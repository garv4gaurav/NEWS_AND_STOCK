{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOI_part4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9jL549pQpZc",
        "outputId": "4427494a-74ae-4fda-d010-7aeb5de6aded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import time as t_\n",
        "import re\n",
        "import requests\n",
        "import sys\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from openpyxl import Workbook\n",
        "import xlrd\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "class Main():\n",
        "    def __init__(self):\n",
        "        TOI_ARCH_URL = \"https://timesofindia.indiatimes.com/archive.cms\"\n",
        "        response = requests.get(TOI_ARCH_URL)\n",
        "        if response.status_code == 200:\n",
        "            self.SOUP = BeautifulSoup(response.content, 'lxml')\n",
        "        else:\n",
        "            raise Exception('Cannot reach the website.')\n",
        "\n",
        "    def fetching_month_urls(self):\n",
        "        start1 = time()\n",
        "        workbook = Workbook()\n",
        "        sheet = workbook.active\n",
        "        sheet['A1'] = 'ORGINAL URL'\n",
        "        sheet['B1'] = 'MONTH URL'\n",
        "        sheet['C1'] = 'TIME'\n",
        "        count = 2\n",
        "\n",
        "        month_urls = []\n",
        "        for tag in self.SOUP.find_all('a', class_='normtxt'):\n",
        "            month_urls.append(\"https://timesofindia.indiatimes.com\" + tag.attrs['href'])\n",
        "            sheet['A' + str(count)] = \"https://timesofindia.indiatimes.com/archive.cms\"\n",
        "            sheet['B' + str(count)] = \"https://timesofindia.indiatimes.com\" + tag.attrs['href']\n",
        "            sheet['C' + str(count)] = str( time()-start1 )\n",
        "            count += 1\n",
        "            \n",
        "        workbook.save('/content/drive/My Drive/'+ 'fetching_month_urls' + '.xlsx')\n",
        "        print(\"fetching month urls done\")\n",
        "        return month_urls\n",
        "\n",
        "    def fetching_days_in_month_urls(self):\n",
        "        start1 = time()\n",
        "        workbook = Workbook()\n",
        "        sheet = workbook.active\n",
        "        sheet['A1'] = 'ORIGINAL URL'\n",
        "        sheet['B1'] = 'DAY URL'\n",
        "        sheet['C1'] = 'TIME'\n",
        "        count = 2\n",
        "        month_urls = self.reading_excel(\"fetching_month_urls.xlsx\")\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        options.add_argument('--ignore-certificate-errors')\n",
        "        options.add_argument('--incognito')\n",
        "        options.add_argument('--log-level=3')\n",
        "        #driver = webdriver.Chrome(\"C:\\Program Files (x86)\\chromedriver.exe\", chrome_options=options)\n",
        "        driver = webdriver.Chrome('chromedriver',options=options)\n",
        "\n",
        "\n",
        "        for url in month_urls:\n",
        "            date_link_dict = {}\n",
        "            #print(url)\n",
        "            driver.get(url)\n",
        "            temp_year = driver.find_element_by_xpath(\"//tbody/tr[2]/td[1]/div[2]/a[2]\")\n",
        "            temp_month = driver.find_element_by_xpath(\"//tbody/tr[2]/td[1]/div[2]/b[1]\")\n",
        "            table = driver.find_element_by_id(\"calender\")\n",
        "            temp1 = table.find_element_by_tag_name(\"tbody\")\n",
        "            temp2 = temp1.find_elements_by_tag_name(\"tr\")\n",
        "            for temp in temp2:\n",
        "                temp3 = temp.find_elements_by_tag_name(\"td\")\n",
        "                for in_cal in temp3:\n",
        "                    try:\n",
        "                        if int(in_cal.text):\n",
        "                            links = in_cal.find_elements_by_tag_name(\"a\")\n",
        "                            for link in links:\n",
        "                                if link.get_attribute(\"href\") != \"\":\n",
        "                                    sheet['A' + str(count)] = url\n",
        "                                    sheet['B' + str(count)] = link.get_attribute(\"href\")\n",
        "                                    sheet['C' + str(count)] = str( time()-start1 )\n",
        "                                    count += 1\n",
        "                    except Exception as err:\n",
        "                        pass\n",
        "                        \n",
        "        driver.quit()\n",
        "        print(\"fetching_days_in_month_urls completed.\")\n",
        "        workbook.save('/content/drive/My Drive/'+ 'fetching_days_in_month_urls2' + '.xlsx')\n",
        "\n",
        "\n",
        "    def reading_excel(self, filename):\n",
        "        all_urls = []\n",
        "        wb = xlrd.open_workbook('/content/drive/My Drive/'+ filename)\n",
        "        sheet = wb.sheet_by_index(0)\n",
        "        no_rows = sheet.nrows\n",
        "        print(no_rows)\n",
        "        #print(sheet.cell_value(0, 0))\n",
        "        for i in range(no_rows-1):\n",
        "            #print(\"_\"+sheet.cell_value(i, 0))\n",
        "            url = sheet.cell_value(i+1, 1)\n",
        "            all_urls.append(url)\n",
        "        return all_urls\n",
        "\n",
        "\n",
        "\n",
        "    def fetch_para_page_link(self):\n",
        "        file_status = open('/content/drive/My Drive/'+'Status.txt', 'a')\n",
        "        start1 = time()\n",
        "        month_urls = self.reading_excel('fetching_days_in_month_urls2.xlsx')\n",
        "        error_occurred = False\n",
        "\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        options.add_argument('--ignore-certificate-errors')\n",
        "        options.add_argument('--incognito')\n",
        "        options.add_argument('--log-level=3')\n",
        "        driver = webdriver.Chrome('chromedriver',options=options)\n",
        "        len_count = 0\n",
        "        month_urls_len = len(month_urls)\n",
        "\n",
        "\n",
        "        #for url in month_urls:\n",
        "        for i in range(month_urls_len):\n",
        "            if i < 3600:\n",
        "                url = month_urls[i]\n",
        "                error_occurred = False\n",
        "                if len_count%100 == 0:\n",
        "                  print(len_count, time()-start1)\n",
        "                while error_occurred == False:\n",
        "                    try:\n",
        "                        driver.get(url)\n",
        "\n",
        "                        tbody = driver.find_elements_by_xpath(\"/html[1]/body[1]/div[1]/table[2]/tbody[1]/tr[2]/td[1]/div[3]/table[1]/tbody[1]\")\n",
        "                        tr = tbody[0].find_elements_by_tag_name(\"tr\")\n",
        "                        for tr_ in tr:\n",
        "                            span = tr_.find_elements_by_tag_name(\"span\")\n",
        "                            for span_ in span:\n",
        "                                links_list = span_.find_elements_by_tag_name(\"a\")\n",
        "                                for link in links_list:\n",
        "                                    #print(\"_\" + link.get_attribute(\"href\"))\n",
        "                                    '''sheet['A' + str(count)] = url\n",
        "                                    sheet['B' + str(count)] = link.get_attribute(\"href\")\n",
        "                                    sheet['C' + str(count)] = str( time()-start1 )\n",
        "                                    count += 1'''\n",
        "                                    file_status.write(url + \"\\t\" + link.get_attribute(\"href\") + \"\\t\" + \"\\n\")\n",
        "                        error_occurred = True\n",
        "\n",
        "                    except Exception as err:\n",
        "                        error_occurred = False\n",
        "                        driver.quit()\n",
        "                        options = webdriver.ChromeOptions()\n",
        "                        options.add_argument('--headless')\n",
        "                        options.add_argument('--no-sandbox')\n",
        "                        options.add_argument('--disable-dev-shm-usage')\n",
        "                        options.add_argument('--ignore-certificate-errors')\n",
        "                        options.add_argument('--incognito')\n",
        "                        options.add_argument('--log-level=3')\n",
        "                        driver = webdriver.Chrome('chromedriver',options=options)\n",
        "                        print(\"error occured.\", err)\n",
        "\n",
        "                len_count += 1\n",
        "            else:\n",
        "              break          \n",
        "\n",
        "        #workbook.save('/content/drive/My Drive/'+ 'fetching_days_in_month_urls3' + '.xlsx')\n",
        "        file_status.close()\n",
        "\n",
        "\n",
        "    def fetch_para(self):\n",
        "        df1 = pd.read_csv('/content/drive/My Drive/Link_full.txt', sep = '\\t')\n",
        "        df1.columns = ['ALL_LINK_PAGE', 'PARA_PAGE_LINK']\n",
        "\n",
        "        print(df1.shape)\n",
        "        len_link1 = df1.shape[0]\n",
        "\n",
        "        file_status = open('/content/drive/My Drive/'+'Business_paragraph.txt', 'a')\n",
        "        file_new_link = open('/content/drive/My Drive/'+'no_business_structure.txt', 'a')\n",
        "        start1 = time()\n",
        "\n",
        "        for i in range(len_link1):\n",
        "            if i%1000 == 0:\n",
        "                print( (len_link1-i), (time()-start1))\n",
        "            url = df1[\"PARA_PAGE_LINK\"][i]\n",
        "            #print(i, \"_\"+url)\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                SOUP = BeautifulSoup(response.content, 'lxml')\n",
        "                para_list = SOUP.find_all(\"div\", class_= \"ga-headlines\")\n",
        "                if len(para_list)>0:\n",
        "                  heading = SOUP.find_all(\"h1\", class_ = \"_23498\")\n",
        "                  date = SOUP.find_all(\"div\", class_ = \"_3Mkg- byline\")\n",
        "                  file_status.write(url + \"\\t\" + date[0].text + \"\\t\" + heading[0].text + \"\\t\" + para_list[0].text + \"\\n\")\n",
        "                else:\n",
        "                  file_new_link.write(df1['ALL_LINK_PAGE'][i] + url + \"\\n\")\n",
        "            else:\n",
        "              pass\n",
        "        file_status.close()\n",
        "        file_new_link.close()\n",
        "\n",
        "\n",
        "main = Main()\n",
        "#main.fetching_month_urls()\n",
        "#main.fetching_days_in_month_urls()\n",
        "#main.fetch_para_page_link()\n",
        "main.fetch_para()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (85.0.4183.121-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(1789816, 3)\n",
            "(788392, 3)\n",
            "1789816 6.67572021484375e-06\n",
            "1779816 27340.739327907562\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}